
epoch: 2, loss: 0.69, time: 1.21:   0%|â–Ž                                                              | 2/400 [00:03<10:44,  1.62s/it]
Traceback (most recent call last):
  File "src/main.py", line 86, in <module>
    main()
  File "src/main.py", line 67, in main
    trained_model = run_server(dataset, num_clients=args.c, epochs=args.epochs,
  File "/home/abenzaamia/NeuCF/federated/src/server.py", line 35, in run_server
    trained_weights = training_process(server_model, clients, num_clients, epochs, local_epochs, dataset, args)
  File "/home/abenzaamia/NeuCF/federated/src/federeco/train.py", line 66, in training_process
    w, loss = single_train_round(server_model, clients, local_epochs)
  File "/home/abenzaamia/NeuCF/federated/src/federeco/train.py", line 110, in single_train_round
    weights, loss = client.train(server_model_copy, local_epochs)
  File "/home/abenzaamia/NeuCF/federated/src/client.py", line 51, in train
    optimizer.step()
  File "/home/abenzaamia/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/home/abenzaamia/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/abenzaamia/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 141, in step
    adam(
  File "/home/abenzaamia/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 281, in adam
    func(params,
  File "/home/abenzaamia/.venv/lib/python3.8/site-packages/torch/optim/adam.py", line 344, in _single_tensor_adam
    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
KeyboardInterrupt